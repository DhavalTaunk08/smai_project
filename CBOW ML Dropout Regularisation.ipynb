{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7129363e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-27T13:39:40.100619Z",
     "iopub.status.busy": "2021-11-27T13:39:40.097692Z",
     "iopub.status.idle": "2021-11-27T13:39:45.510044Z",
     "shell.execute_reply": "2021-11-27T13:39:45.509398Z",
     "shell.execute_reply.started": "2021-11-27T13:34:29.669569Z"
    },
    "papermill": {
     "duration": 5.435369,
     "end_time": "2021-11-27T13:39:45.510212",
     "exception": false,
     "start_time": "2021-11-27T13:39:40.074843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa981eee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-27T13:39:45.539763Z",
     "iopub.status.busy": "2021-11-27T13:39:45.539259Z",
     "iopub.status.idle": "2021-11-27T13:39:46.766976Z",
     "shell.execute_reply": "2021-11-27T13:39:46.766492Z",
     "shell.execute_reply.started": "2021-11-27T13:10:42.402545Z"
    },
    "papermill": {
     "duration": 1.24347,
     "end_time": "2021-11-27T13:39:46.767141",
     "exception": false,
     "start_time": "2021-11-27T13:39:45.523671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/quora-ques-pair/test_data.csv/test_data.csv').fillna('')\n",
    "val = pd.read_csv('../input/quora-ques-pair/val_data.csv/val_data.csv').fillna('')\n",
    "test = pd.read_csv('../input/quora-ques-pair/test_data.csv/test_data.csv').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c9ee56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-27T13:39:46.793631Z",
     "iopub.status.busy": "2021-11-27T13:39:46.792950Z",
     "iopub.status.idle": "2021-11-27T13:39:46.795498Z",
     "shell.execute_reply": "2021-11-27T13:39:46.795872Z",
     "shell.execute_reply.started": "2021-11-27T13:10:43.715098Z"
    },
    "papermill": {
     "duration": 0.017601,
     "end_time": "2021-11-27T13:39:46.795999",
     "exception": false,
     "start_time": "2021-11-27T13:39:46.778398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word2vec_file = '../input/d/sandreds/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a62dc09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-27T13:39:46.827484Z",
     "iopub.status.busy": "2021-11-27T13:39:46.826713Z",
     "iopub.status.idle": "2021-11-27T13:39:46.839866Z",
     "shell.execute_reply": "2021-11-27T13:39:46.840300Z",
     "shell.execute_reply.started": "2021-11-27T13:10:43.722268Z"
    },
    "papermill": {
     "duration": 0.033588,
     "end_time": "2021-11-27T13:39:46.840430",
     "exception": false,
     "start_time": "2021-11-27T13:39:46.806842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_preprocessed</th>\n",
       "      <th>question2_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>204673</td>\n",
       "      <td>93885</td>\n",
       "      <td>307635</td>\n",
       "      <td>If there is a God, where is He!</td>\n",
       "      <td>Why is god a \"He\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>if there is a god , where is he !</td>\n",
       "      <td>why is god a `` he '' ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17716</td>\n",
       "      <td>2093</td>\n",
       "      <td>15628</td>\n",
       "      <td>Do you believe that everything happens for a r...</td>\n",
       "      <td>Does everything happen for a reason?</td>\n",
       "      <td>1</td>\n",
       "      <td>do you believe that everything happens for a r...</td>\n",
       "      <td>does everything happen for a reason ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>291767</td>\n",
       "      <td>352623</td>\n",
       "      <td>413255</td>\n",
       "      <td>Will there always be web hosting that will sup...</td>\n",
       "      <td>Will there always be web hosting that supports...</td>\n",
       "      <td>1</td>\n",
       "      <td>will there always be web hosting that will sup...</td>\n",
       "      <td>will there always be web hosting that supports...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>203758</td>\n",
       "      <td>59824</td>\n",
       "      <td>67971</td>\n",
       "      <td>What is the proof of Indian Army's surgical st...</td>\n",
       "      <td>Has India provided any proof of the surgical s...</td>\n",
       "      <td>1</td>\n",
       "      <td>what is the proof of indian army 's surgical s...</td>\n",
       "      <td>has india provided any proof of the surgical s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41747</td>\n",
       "      <td>75326</td>\n",
       "      <td>75327</td>\n",
       "      <td>What do Indian Muslims think of Modi?</td>\n",
       "      <td>What do Indian Muslim think about PM Narendra ...</td>\n",
       "      <td>1</td>\n",
       "      <td>what do indian muslims think of modi ?</td>\n",
       "      <td>what do indian muslim think about pm narendra ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  204673   93885  307635                    If there is a God, where is He!   \n",
       "1   17716    2093   15628  Do you believe that everything happens for a r...   \n",
       "2  291767  352623  413255  Will there always be web hosting that will sup...   \n",
       "3  203758   59824   67971  What is the proof of Indian Army's surgical st...   \n",
       "4   41747   75326   75327              What do Indian Muslims think of Modi?   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0                                 Why is god a \"He\"?             0   \n",
       "1               Does everything happen for a reason?             1   \n",
       "2  Will there always be web hosting that supports...             1   \n",
       "3  Has India provided any proof of the surgical s...             1   \n",
       "4  What do Indian Muslim think about PM Narendra ...             1   \n",
       "\n",
       "                              question1_preprocessed  \\\n",
       "0                  if there is a god , where is he !   \n",
       "1  do you believe that everything happens for a r...   \n",
       "2  will there always be web hosting that will sup...   \n",
       "3  what is the proof of indian army 's surgical s...   \n",
       "4             what do indian muslims think of modi ?   \n",
       "\n",
       "                              question2_preprocessed  \n",
       "0                            why is god a `` he '' ?  \n",
       "1              does everything happen for a reason ?  \n",
       "2  will there always be web hosting that supports...  \n",
       "3  has india provided any proof of the surgical s...  \n",
       "4  what do indian muslim think about pm narendra ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9f6d2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-27T13:39:46.870196Z",
     "iopub.status.busy": "2021-11-27T13:39:46.869680Z",
     "iopub.status.idle": "2021-11-27T13:40:56.405543Z",
     "shell.execute_reply": "2021-11-27T13:40:56.404933Z",
     "shell.execute_reply.started": "2021-11-27T13:10:43.746605Z"
    },
    "papermill": {
     "duration": 69.55394,
     "end_time": "2021-11-27T13:40:56.405691",
     "exception": false,
     "start_time": "2021-11-27T13:39:46.851751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def buildVocabulary(reviews):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=False, split=' ')\n",
    "    tokenizer.fit_on_texts(reviews)\n",
    "    return tokenizer\n",
    "\n",
    "def getSequences(reviews, tokenizer, seq_maxlen):\n",
    "    reviews_seq = tokenizer.texts_to_sequences(reviews)\n",
    "    return np.array(tf.keras.preprocessing.sequence.pad_sequences(reviews_seq, maxlen=seq_maxlen))\n",
    "\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_file, binary = True)\n",
    "\n",
    "def getEmbeddingWeightMatrix(word2idx):    \n",
    "    embedding_matrix = np.zeros((len(word2idx)+1, 300))\n",
    "    for word, i in tqdm(word2idx.items()):\n",
    "       \n",
    "        embedding_vector = word2vec_model[word] if word in word2vec_model else np.random.rand(1,300)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c97283f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-27T13:40:56.460244Z",
     "iopub.status.busy": "2021-11-27T13:40:56.459548Z",
     "iopub.status.idle": "2021-11-27T13:41:10.200708Z",
     "shell.execute_reply": "2021-11-27T13:41:10.200210Z",
     "shell.execute_reply.started": "2021-11-27T13:11:45.518534Z"
    },
    "papermill": {
     "duration": 13.783239,
     "end_time": "2021-11-27T13:41:10.200838",
     "exception": false,
     "start_time": "2021-11-27T13:40:56.417599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67043\n"
     ]
    }
   ],
   "source": [
    "tokenizer = buildVocabulary(train['question1'].tolist()+train['question2'].tolist()+val['question1'].tolist()+val['question2'].tolist()+test['question1'].tolist()+test['question2'].tolist())\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "\n",
    "x_train1 = getSequences(train['question1'], tokenizer, 200)\n",
    "x_train2 = getSequences(train['question2'], tokenizer, 200)\n",
    "y_train = tf.keras.utils.to_categorical(train['is_duplicate'])\n",
    "\n",
    "x_val1 = getSequences(val['question1'], tokenizer, 200)\n",
    "x_val2 = getSequences(val['question2'], tokenizer, 200)\n",
    "y_val = tf.keras.utils.to_categorical(val['is_duplicate'])\n",
    "\n",
    "x_test1 = getSequences(test['question1'], tokenizer, 200)\n",
    "x_test2 = getSequences(test['question2'], tokenizer, 200)\n",
    "y_test = tf.keras.utils.to_categorical(test['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580d251a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-27T13:41:10.228748Z",
     "iopub.status.busy": "2021-11-27T13:41:10.228020Z",
     "iopub.status.idle": "2021-11-27T13:41:10.705699Z",
     "shell.execute_reply": "2021-11-27T13:41:10.704306Z",
     "shell.execute_reply.started": "2021-11-27T13:12:14.356043Z"
    },
    "papermill": {
     "duration": 0.493073,
     "end_time": "2021-11-27T13:41:10.705828",
     "exception": false,
     "start_time": "2021-11-27T13:41:10.212755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67042/67042 [00:00<00:00, 143587.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67043, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#embedding_vectors = loadGloveWordEmbeddings()\n",
    "#print(len(embedding_vectors))\n",
    "\n",
    "embedding_weight_matrix = getEmbeddingWeightMatrix(tokenizer.word_index)\n",
    "print(embedding_weight_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "376d9fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-27T13:41:10.744344Z",
     "iopub.status.busy": "2021-11-27T13:41:10.743779Z",
     "iopub.status.idle": "2021-11-27T13:41:13.935711Z",
     "shell.execute_reply": "2021-11-27T13:41:13.934772Z",
     "shell.execute_reply.started": "2021-11-27T13:27:27.669498Z"
    },
    "papermill": {
     "duration": 3.215972,
     "end_time": "2021-11-27T13:41:13.935848",
     "exception": false,
     "start_time": "2021-11-27T13:41:10.719876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 13:41:10.832116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-27 13:41:10.981618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-27 13:41:10.982427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-27 13:41:10.983858: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-27 13:41:10.985130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-27 13:41:10.985799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-27 13:41:10.986424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-27 13:41:13.311232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-27 13:41:13.311964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-27 13:41:13.312674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-27 13:41:13.313255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "#he_initializer = tf.keras.initializers.HeUniform()\n",
    "inp1 = tf.keras.Input(shape=(x_train1.shape[1],))\n",
    "inp2 = tf.keras.Input(shape=(x_train2.shape[1],))\n",
    "\n",
    "inner1= tf.keras.layers.Embedding(input_dim=67043, output_dim=300, input_length=200, \n",
    "                                  weights=[embedding_weight_matrix], trainable=True)(inp1)\n",
    "inner2= tf.keras.layers.Embedding(input_dim=67043, output_dim=300, input_length=200,\n",
    "                                  weights=[embedding_weight_matrix], trainable=True)(inp2)\n",
    " \n",
    "inner = tf.keras.layers.concatenate([inner1+inner2, inner1-inner2, tf.math.multiply(inner1, inner2)], axis=-1)\n",
    "\n",
    "inner = tf.keras.backend.sum(inner, axis=1, keepdims=False)\n",
    "inner = tf.keras.layers.Dense(300, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))(inner)\n",
    "inner = tf.keras.layers.Dropout(0.2)(inner)\n",
    "inner = tf.keras.layers.Dense(200, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))(inner)\n",
    "inner = tf.keras.layers.Dropout(0.2)(inner)\n",
    "inner = tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))(inner)\n",
    "inner = tf.keras.layers.Dropout(0.2)(inner)\n",
    "output = tf.keras.layers.Dense(2, activation='softmax')(inner)\n",
    "\n",
    "model = tf.keras.Model(inputs = [inp1, inp2], outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7dba2d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-27T13:41:13.973100Z",
     "iopub.status.busy": "2021-11-27T13:41:13.972255Z",
     "iopub.status.idle": "2021-11-27T13:41:13.987912Z",
     "shell.execute_reply": "2021-11-27T13:41:13.988527Z",
     "shell.execute_reply.started": "2021-11-27T13:27:28.324190Z"
    },
    "papermill": {
     "duration": 0.037826,
     "end_time": "2021-11-27T13:41:13.988697",
     "exception": false,
     "start_time": "2021-11-27T13:41:13.950871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 300)     20112900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     20112900    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 200, 300)     0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, 200, 300)     0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 200, 300)     0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 200, 900)     0           tf.__operators__.add[0][0]       \n",
      "                                                                 tf.math.subtract[0][0]           \n",
      "                                                                 tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum (TFOpLambda) (None, 900)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 300)          270300      tf.math.reduce_sum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          60200       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          20100       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            202         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 40,576,602\n",
      "Trainable params: 40,576,602\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = \"adam\", loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c0c86ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-27T13:41:14.024992Z",
     "iopub.status.busy": "2021-11-27T13:41:14.024188Z",
     "iopub.status.idle": "2021-11-27T13:42:26.517456Z",
     "shell.execute_reply": "2021-11-27T13:42:26.518855Z",
     "shell.execute_reply.started": "2021-11-27T13:27:29.396799Z"
    },
    "papermill": {
     "duration": 72.514821,
     "end_time": "2021-11-27T13:42:26.519128",
     "exception": false,
     "start_time": "2021-11-27T13:41:14.004307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 13:41:14.139989: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "632/632 [==============================] - 23s 33ms/step - loss: 1.4585 - accuracy: 0.6716 - val_loss: 0.5944 - val_accuracy: 0.7091\n",
      "Epoch 2/4\n",
      "632/632 [==============================] - 16s 25ms/step - loss: 0.5432 - accuracy: 0.7499 - val_loss: 0.5638 - val_accuracy: 0.7303\n",
      "Epoch 3/4\n",
      "632/632 [==============================] - 15s 24ms/step - loss: 0.4664 - accuracy: 0.7947 - val_loss: 0.6222 - val_accuracy: 0.7380\n",
      "Epoch 4/4\n",
      "632/632 [==============================] - 15s 24ms/step - loss: 0.4202 - accuracy: 0.8218 - val_loss: 0.5795 - val_accuracy: 0.7284\n"
     ]
    }
   ],
   "source": [
    "save_weights = tf.keras.callbacks.ModelCheckpoint('cbow_mlp.h5', monitor='val_loss', save_best_only=True)\n",
    "checkpoint_filepath = 'weights.best.{epoch:01d}.hdf5'\n",
    "#model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "#verbose = 1,\n",
    "#monitor = 'val_loss',\n",
    "#save_best_only = False)\n",
    "history = model.fit((x_train1, x_train2), y_train,\n",
    "                    batch_size = 64,\n",
    "                    validation_data = ((x_val1, x_val2), y_val),\n",
    "                    validation_batch_size = 32,\n",
    "                    epochs=4, \n",
    "                    callbacks=[save_weights], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e786b68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-27T13:42:28.244754Z",
     "iopub.status.busy": "2021-11-27T13:42:28.243805Z",
     "iopub.status.idle": "2021-11-27T13:43:09.569995Z",
     "shell.execute_reply": "2021-11-27T13:43:09.570512Z",
     "shell.execute_reply.started": "2021-11-27T13:31:52.063332Z"
    },
    "papermill": {
     "duration": 41.528106,
     "end_time": "2021-11-27T13:43:09.570664",
     "exception": false,
     "start_time": "2021-11-27T13:42:28.042558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10108/10108 [==============================] - 23s 2ms/step - loss: 0.3454 - accuracy: 0.8793\n",
      "loss on test data is 0.345432311296463\n",
      "accuracy on test data is 0.8792945742607117\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate((x_test1, x_test2), y_test, batch_size=4, verbose=1)\n",
    "\n",
    "print('loss on test data is', loss)\n",
    "print('accuracy on test data is', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c84c1ff4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-27T13:43:10.313909Z",
     "iopub.status.busy": "2021-11-27T13:43:10.306261Z",
     "iopub.status.idle": "2021-11-27T13:43:11.829214Z",
     "shell.execute_reply": "2021-11-27T13:43:11.829678Z",
     "shell.execute_reply.started": "2021-11-27T13:33:47.897210Z"
    },
    "papermill": {
     "duration": 1.948284,
     "end_time": "2021-11-27T13:43:11.829824",
     "exception": false,
     "start_time": "2021-11-27T13:43:09.881540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score on test is 0.8467144113582108\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict((x_test1, x_test2))\n",
    "print('F1_score on test is', f1_score(np.argmax(pred, axis=1), np.argmax(y_test, axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9929df87",
   "metadata": {
    "papermill": {
     "duration": 0.308571,
     "end_time": "2021-11-27T13:43:12.450411",
     "exception": false,
     "start_time": "2021-11-27T13:43:12.141840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 223.597372,
   "end_time": "2021-11-27T13:43:15.659790",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-27T13:39:32.062418",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
